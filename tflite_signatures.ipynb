{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://www.tensorflow.org/lite/guide/signatures\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.Module):\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.float32)])\n",
    "  def encode(self, x):\n",
    "    result = tf.strings.as_string(x)\n",
    "    return {\n",
    "         \"encoded_result\": result\n",
    "    }\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(shape=[None], dtype=tf.string)])\n",
    "  def decode(self, x):\n",
    "    result = tf.strings.to_number(x)\n",
    "    return {\n",
    "         \"decoded_result\": result\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert a model from Signatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. From Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/coding/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: saved_models/coding/assets\n",
      "2022-07-09 12:35:44.822882: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-07-09 12:35:44.822905: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs\n",
      "{'decode': {'inputs': ['x'], 'outputs': ['decoded_result']}, 'encode': {'inputs': ['x'], 'outputs': ['encoded_result']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-09 12:35:44.823090: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: saved_models/coding\n",
      "2022-07-09 12:35:44.823303: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-07-09 12:35:44.823313: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: saved_models/coding\n",
      "2022-07-09 12:35:44.823746: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-07-09 12:35:44.831677: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: saved_models/coding\n",
      "2022-07-09 12:35:44.837621: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 14532 microseconds.\n",
      "2022-07-09 12:35:44.857130: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1892] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexAsString, FlexStringToNumber\n",
      "Details:\n",
      "\ttf.AsString(tensor<?xf32>) -> (tensor<?x!tf_type.string>) : {device = \"\", fill = \"\", precision = -1 : i64, scientific = false, shortest = false, width = -1 : i64}\n",
      "\ttf.StringToNumber(tensor<?x!tf_type.string>) -> (tensor<?xf32>) : {device = \"\", out_type = f32}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "2022-07-09 12:35:44.857158: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs\n",
      "\n",
      "2022-07-09 12:35:44.860435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9876 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:26:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "# Save the model\n",
    "SAVED_MODEL_PATH = 'saved_models/coding'\n",
    "\n",
    "tf.saved_model.save(\n",
    "    model, SAVED_MODEL_PATH,\n",
    "    signatures={\n",
    "      'encode': model.encode.get_concrete_function(),\n",
    "      'decode': model.decode.get_concrete_function()\n",
    "    })\n",
    "\n",
    "# Convert the saved model using TFLiteConverter\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Print the signatures from the converted model\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "signatures = interpreter.get_signature_list()\n",
    "print(signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. From Keras Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmph8w6ub7v/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmph8w6ub7v/assets\n",
      "2022-07-09 12:37:51.337908: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-07-09 12:37:51.337928: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2022-07-09 12:37:51.338119: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmph8w6ub7v\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'serving_default': {'inputs': ['x_input'], 'outputs': ['output']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-09 12:37:51.338706: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-07-09 12:37:51.338716: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmph8w6ub7v\n",
      "2022-07-09 12:37:51.340299: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-07-09 12:37:51.349780: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmph8w6ub7v\n",
      "2022-07-09 12:37:51.353960: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 15841 microseconds.\n"
     ]
    }
   ],
   "source": [
    "# Generate a Keras model.\n",
    "keras_model = tf.keras.Sequential(\n",
    "    [\n",
    "        tf.keras.layers.Dense(2, input_dim=4, activation='relu', name='x'),\n",
    "        tf.keras.layers.Dense(1, activation='relu', name='output'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Convert the keras model using TFLiteConverter.\n",
    "# Keras model converter API uses the default signature automatically.\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Print the signatures from the converted model\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "\n",
    "signatures = interpreter.get_signature_list()\n",
    "print(signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. From Concreate Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpio1qddgg/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpio1qddgg/assets\n",
      "2022-07-09 12:38:19.129906: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2022-07-09 12:38:19.129926: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs\n",
      "{'decode': {'inputs': ['x'], 'outputs': ['decoded_result']}, 'encode': {'inputs': ['x'], 'outputs': ['encoded_result']}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-09 12:38:19.130111: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpio1qddgg\n",
      "2022-07-09 12:38:19.130346: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2022-07-09 12:38:19.130355: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpio1qddgg\n",
      "2022-07-09 12:38:19.130882: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2022-07-09 12:38:19.139398: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpio1qddgg\n",
      "2022-07-09 12:38:19.142520: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 12409 microseconds.\n",
      "2022-07-09 12:38:19.159427: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1892] TFLite interpreter needs to link Flex delegate in order to run the model since it contains the following Select TFop(s):\n",
      "Flex ops: FlexAsString, FlexStringToNumber\n",
      "Details:\n",
      "\ttf.AsString(tensor<?xf32>) -> (tensor<?x!tf_type.string>) : {device = \"\", fill = \"\", precision = -1 : i64, scientific = false, shortest = false, width = -1 : i64}\n",
      "\ttf.StringToNumber(tensor<?x!tf_type.string>) -> (tensor<?xf32>) : {device = \"\", out_type = f32}\n",
      "See instructions: https://www.tensorflow.org/lite/guide/ops_select\n",
      "2022-07-09 12:38:19.159445: I tensorflow/compiler/mlir/lite/flatbuffer_export.cc:1963] Estimated count of arithmetic ops: 0  ops, equivalently 0  MACs\n",
      "\n",
      "2022-07-09 12:38:19.163389: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9876 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:26:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "\n",
    "# Convert the concrete functions using TFLiteConverter\n",
    "converter = tf.lite.TFLiteConverter.from_concrete_functions(\n",
    "    [model.encode.get_concrete_function(),\n",
    "     model.decode.get_concrete_function()], model)\n",
    "converter.target_spec.supported_ops = [\n",
    "    tf.lite.OpsSet.TFLITE_BUILTINS,  # enable TensorFlow Lite ops.\n",
    "    tf.lite.OpsSet.SELECT_TF_OPS  # enable TensorFlow ops.\n",
    "]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Print the signatures from the converted model\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "signatures = interpreter.get_signature_list()\n",
    "print(signatures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Signatures\n",
    "\n",
    "TensorFlow inference APIs support the signature-based executions:   \n",
    "   \n",
    "* Accessing the input/output tensors through the names of the inputs and outputs, specified by the signature.\n",
    "* Running each entry point of the graph separately, identified by the signature key.\n",
    "* Support for the SavedModel's initialization procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signature: {'decode': {'inputs': ['x'], 'outputs': ['decoded_result']}, 'encode': {'inputs': ['x'], 'outputs': ['encoded_result']}}\n",
      "Input: tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)\n",
      "Encoded result: {'encoded_result': array([b'1.000000', b'2.000000', b'3.000000'], dtype=object)}\n",
      "Decoded result: {'decoded_result': array([1., 2., 3.], dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-09 12:41:00.880310: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9876 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:26:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "# Load the TFLite model in TFLite Interpreter\n",
    "interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "\n",
    "# Print the signatures from the converted model\n",
    "signatures = interpreter.get_signature_list()\n",
    "print('Signature:', signatures)\n",
    "\n",
    "# encode and decode are callable with input as arguments.\n",
    "encode = interpreter.get_signature_runner('encode')\n",
    "decode = interpreter.get_signature_runner('decode')\n",
    "\n",
    "# 'encoded' and 'decoded' are dictionaries with all outputs from the inference.\n",
    "input = tf.constant([1, 2, 3], dtype=tf.float32)\n",
    "print('Input:', input)\n",
    "encoded = encode(x=input)\n",
    "print('Encoded result:', encoded)\n",
    "decoded = decode(x=encoded['encoded_result'])\n",
    "print('Decoded result:', decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1ec7e7a7bda2752b69488ef6b463cd212a85bea9beda62a0e3d72f3155be411c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
